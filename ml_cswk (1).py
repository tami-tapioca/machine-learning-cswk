# -*- coding: utf-8 -*-
"""ML_CSWK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ClDfkJLu_75oazMUblPngh7CtvHI6tZ
"""

import numpy as np
import pandas as pd # for data analysis
import seaborn as sns # for vizualisation
import matplotlib.pyplot as plt # for vizualisation
import matplotlib.pylab as pylab
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.tree import DecisionTreeRegressor, export_text
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor

from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, make_scorer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder

# read the data (import file into local folder)
data = pd.read_csv('CW1_train.csv')
data.head()

# how many rows and columns in the dataset
data.shape

# checks if theres any null values in the dataset
# can see which columns are categorical, numerical, etc
data.info()

# categorizing features
categorical_features = ['cut', 'color', 'clarity']
numerical_features = ['carat', 'depth', 'price', 'table']
xyz_features = ['x', 'y', 'z']
a_features = ['a1', 'a2', 'a3', 'a4', 'a5']
b_features = ['b1', 'b2', 'b3', 'b4', 'b5']
a2_features = ['a6', 'a7', 'a8', 'a9', 'a10']
b2_features = ['b6', 'b7', 'b8', 'b9', 'b10']

data[numerical_features + xyz_features + ['outcome']].describe()
# x, y and z have minimum values of 0 - possible outliers

data[a_features + b_features].describe()

data[a2_features + b2_features].describe()

# x, y and z have min values of 0 -> suggests they are faulty
# drop xyz values when they are 0
data = data.drop(data[data['x']==0].index)
data = data.drop(data[data['y']==0].index)
data = data.drop(data[data['z']==0].index)
data.shape

"""data visualization"""

# shows comparisons with every available feature
shade = ["#6D2491", "#53067D", "#820E9E", "#A1068F", "#B0098C"]
ax=sns.pairplot(data, x_vars=data.columns.drop("outcome"), y_vars=["outcome"], hue="cut", palette=shade)

# individual colours separated by cut e.g ideal, good, premium, etc
# not really any correlation with outcome and any of the other features (numerical)

ax = sns.regplot(x="outcome", y="x", data=data, fit_reg=True, scatter_kws={"color": "#430C94"}, line_kws={"color": "#E34FD2"})
ax.set_title("Regression Line on outcome vs 'x'", color="#940C82")

ax = sns.regplot(x="outcome", y="y", data=data, fit_reg=True, scatter_kws={"color": "#430C94"}, line_kws={"color": "#E34FD2"})
ax.set_title("Regression Line on outcome vs 'y'", color="#940C82")

ax = sns.regplot(x="outcome", y="z", data=data, fit_reg=True, scatter_kws={"color": "#430C94"}, line_kws={"color": "#E34FD2"})
ax.set_title("Regression Line on outcome vs 'z'", color="#940C82")

ax = sns.regplot(x="outcome", y="carat", data=data, fit_reg=True, scatter_kws={"color": "#430C94"}, line_kws={"color": "#E34FD2"})
ax.set_title("Regression Line on outcome vs carat", color="#940C82")

ax = sns.regplot(x="outcome", y="depth", data=data, fit_reg=True, scatter_kws={"color": "#430C94"}, line_kws={"color": "#E34FD2"})
ax.set_title("Regression Line on outcome vs depth", color="#940C82")

ax = sns.regplot(x="outcome", y="table", data=data, fit_reg=True, scatter_kws={"color": "#430C94"}, line_kws={"color": "#E34FD2"})
ax.set_title("Regression Line on outcome vs table", color="#940C82")

ax = sns.regplot(x="outcome", y="price", data=data, fit_reg=True, scatter_kws={"color": "#430C94"}, line_kws={"color": "#E34FD2"})
ax.set_title("Regression Line on outcome vs price", color="#940C82")

# dropping the outliers observed in the above graphs
data = data[(data["depth"]>52.5)]
data = data[(data["carat"]<3.5)]
data = data[(data["table"]<70)&(data["table"]>45)]
data = data[(data["y"]<50)]
data = data[(data["z"]<7)&(data["z"]>2)]
data.shape

"""dealing with categorical features"""

plt.figure(figsize=(12,8))
shade = ["#6D2491", "#53067D", "#820E9E", "#A1068F", "#B0098C"]
ax = sns.violinplot(x="cut",y="outcome", data=data, palette=shade, scale= "count")
ax.set_title("Violinplot For Cut vs Outcome", color="#BD1585")
ax.set_ylabel("Outcome", color="#BD1585")
ax.set_xlabel("Cut", color="#BD1585")

plt.figure(figsize=(12,8))
shade_1 = ["#370E73","#490E73", "#5A0E73","#690E73","#730E6C", "#730E5B", "#730E47"]
ax = sns.violinplot(x="color",y="outcome", data=data, palette=shade_1,scale= "count")
ax.set_title("Violinplot For Color vs Outcome", color="#BD1585")
ax.set_ylabel("Outcome", color="#BD1585")
ax.set_xlabel("Color", color="#BD1585")

plt.figure(figsize=(12,8))
shade_2 = ["#370E73","#490E73", "#5A0E73","#690E73","#730E6C", "#730E5B", "#730E47", "#730E2B"]
ax = sns.violinplot(x="clarity",y="outcome", data=data, palette=shade_2,scale= "count")
ax.set_title("Violinplot For Clarity vs Outcome", color="#BD1585")
ax.set_ylabel("Outcome", color="#BD1585")
ax.set_xlabel("Clarity", color="#BD1585")

# make copy to avoid changing original data
label_data = data.copy()

# apply label encoder to each column with categorical data, they will be converted to numbers
label_encoder = LabelEncoder()
for col in categorical_features:
    label_data[col] = label_encoder.fit_transform(label_data[col])
label_data.head()

# correlation matrix
features = ['outcome'] + categorical_features + numerical_features + xyz_features

cmap = sns.diverging_palette(70,20,s=50, l=40, n=6,as_cmap=True)
corrmat = label_data[features].corr()
f, ax = plt.subplots(figsize=(12,12))
sns.heatmap(corrmat,cmap=sns.cubehelix_palette(as_cmap=True),annot=True, )

features = ['outcome'] + a_features + b_features

cmap = sns.diverging_palette(70,20,s=50, l=40, n=6,as_cmap=True)
corrmat = label_data[features].corr()
f, ax = plt.subplots(figsize=(12,12))
sns.heatmap(corrmat,cmap=sns.cubehelix_palette(as_cmap=True),annot=True, )

features = ['outcome'] + a2_features + b2_features

cmap = sns.diverging_palette(70,20,s=50, l=40, n=6,as_cmap=True)
corrmat = label_data[features].corr()
f, ax = plt.subplots(figsize=(12,12))
sns.heatmap(corrmat,cmap=sns.cubehelix_palette(as_cmap=True),annot=True, )

"""depth, table, a1, a4, b1 and b3 have the highest correlation with outcome.

considering features with correlations (+/-) 0.10

seeing if there's any outliers in features that have the highest correlation
"""

final_features = ['depth', 'table', 'a1', 'a4', 'b1', 'b3']
for col in final_features:
    fig = plt.figure(figsize=(5, 5))
    ax = fig.gca()
    feature = label_data[col]
    label = label_data['outcome']
    correlation = feature.corr(label)
    plt.scatter(x=feature, y=label)
    plt.xlabel(col)
    plt.ylabel('Outcomes')
    ax.set_title('outcomes vs ' + col + '. correlation: ' + str(correlation))
plt.show()

# no outliers

"""model building"""

X, y = label_data[['depth', 'table', 'a1', 'a4', 'b1', 'b3']].values, label_data['outcome'].values
print('Features:',X[:10], '\nLabels:', y[:10], sep='\n')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

print ('Training Set: %d rows\nTest Set: %d rows' % (X_train.shape[0], X_test.shape[0]))

"""pipelines"""

from sklearn.ensemble import GradientBoostingRegressor

pipeline_xg = Pipeline(steps=[('scaler', StandardScaler()),
                           ('regressor', XGBRegressor())])

pipeline_rf = Pipeline(steps=[('scaler', StandardScaler()),
                           ('regressor', RandomForestRegressor())])

pipeline_dt = Pipeline(steps=[('scaler', StandardScaler()),
                           ('regressor', DecisionTreeRegressor())])

pipeline_knn = Pipeline(steps=[('scaler', StandardScaler()),
                           ('regressor', KNeighborsRegressor())])

pipeline_lr = Pipeline(steps=[('scaler', StandardScaler()),
                           ('regressor', LinearRegression())])

pipeline_lasso = Pipeline(steps=[('scaler', StandardScaler()),
                           ('regressor', Lasso())])

pipeline_gb = Pipeline(steps=[('scaler', StandardScaler()),
                           ('regressor', GradientBoostingRegressor())])

pipelines = [pipeline_lr, pipeline_lasso, pipeline_dt, pipeline_rf, pipeline_knn, pipeline_xg, pipeline_gb]

pipe_dict = {0: "LinearRegression", 1: "Lasso", 2: "DecisionTree", 3: "RandomForest", 4: "KNeighbors", 5: "XGBRegressor", 6: "GradientBoostingRegressor"}

# testing models
for pipe in pipelines:
    pipe.fit(X_train, y_train)

cv_results_rms = []
for i, model in enumerate(pipelines):
    cv_score = cross_val_score(model, X_train,y_train,scoring="r2", cv=10)
    cv_results_rms.append(cv_score)
    print("%s: %f " % (pipe_dict[i], cv_score.mean()))

"""hyperparameter optimization"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, r2_score

# using the best estimator
alg = GradientBoostingRegressor()

# initial hyperparameters
params = {
 'learning_rate': [0.1, 0.5, 1.0],
 'n_estimators' : [50, 100, 150],
 }

# find the best hyperparameter combination to optimize the R2 metric
score = make_scorer(r2_score)
gridsearch = GridSearchCV(alg, params, scoring=score, cv=3, return_train_score=True)
gridsearch.fit(X_train, y_train)
print("Best parameter combination:", gridsearch.best_params_, "\n")

# get the best model
model=gridsearch.best_estimator_
print(model, "\n")

# evaluate the model using the test data
predictions = model.predict(X_test)
r2 = r2_score(y_test, predictions)
print("R2:", r2)

# fine tuning hyperparamters to find the best performing one
# {'learning_rate': 0.1, 'n_estimators': 50}
# {'learning_rate': 0.15, 'n_estimators': 50}
# {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}
# {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 45}
# {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 40}

params = {
 'learning_rate': [0.20],
 'n_estimators' : [40],
 'max_depth' : [3]
 }

# find the best hyperparameter combination to optimize the R2 metric
score = make_scorer(r2_score)
gridsearch = GridSearchCV(alg, params, scoring=score, cv=3, return_train_score=True)
gridsearch.fit(X_train, y_train)
print("Best parameter combination:", gridsearch.best_params_, "\n")

# get the best model
model=gridsearch.best_estimator_
print(model, "\n")

# evaluate the model using the test data
predictions = model.predict(X_test)
r2 = r2_score(y_test, predictions)
print("R2:", r2)

# predicted vs actual ground truths
np.set_printoptions(suppress=True)
print('Predicted labels: ', np.round(predictions)[:10])
print('Actual labels   : ' ,y_test[:10])

# plot predicted vs actual
plt.scatter(y_test, predictions)
plt.xlabel('Actual Labels')
plt.ylabel('Predicted Labels')
plt.title('Outcome Predictions')
# overlay the regression line
z = np.polyfit(y_test, predictions, 1)
p = np.poly1d(z)
plt.plot(y_test,p(y_test), color='magenta')
plt.show()